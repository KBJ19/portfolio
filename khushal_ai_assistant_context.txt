--- KHUSHAL JHAVERI PORTFOLIO CONTEXT FILE ---

üëã About Me
Hi, I'm Khushal Jhaveri ‚Äî currently pursuing my Master‚Äôs in Computer Science with a focus on Artificial Intelligence at the University of Southern California (USC). I like building things that blend AI, vision, and language, especially when they can be applied to the real world in messy, human ways.

Over the past few years, I've been lucky to work on projects that go beyond just accuracy scores. Whether it was building a multimodal emotion detector using facial cues, audio, and text ‚Äî or developing computer vision tools to improve blurry textbook scans for rural education ‚Äî the goal has always been to make AI feel a little more useful, a little more human.

Before USC, I completed my B.Tech in Computer Engineering from K.J. Somaiya College in Mumbai, with an Honors in Cybersecurity. I was also the Marketing Head of BloomBox, our entrepreneurship cell, where I led large-scale events and funding efforts ‚Äî which, by the way, taught me more about leadership than any classroom ever could.

I‚Äôve worked as a Deep Learning Engineer Intern at ResoluteAI, leading proof-of-concept projects for clients involving drone imaging, YOLOv8 models, and traditional computer vision ‚Äî sometimes ditching deep learning altogether when CV worked better. I‚Äôve also interned at Suvidha Foundation, using OpenCV to enhance scanned educational material for underprivileged girls.

Outside of tech, I‚Äôve worn many hats ‚Äî fundraiser, speaker coordinator, pitch deck builder, and event host. I believe all of that shapes how I think about problem-solving today.

üå± Things I'm exploring
Using AI for education and accessibility
Low-cost healthcare tools via computer vision
LLMs for summarization, creativity, and context-aware dialogue
Cloud systems that scale smartly ‚Äî I‚Äôm Azure certified too
üí¨ Outside the Resume
I‚Äôm not just about code and models ‚Äî I love putting myself out there.

I‚Äôve hosted events, built sponsorship decks, run logistics for 1000+ people, and helped organize national startup summits. I enjoy learning how people think, what makes them tick, and how good design and tech can be a bridge between people and systems.

I‚Äôm an extrovert, but also someone who likes going deep ‚Äî whether it‚Äôs a model architecture, a business deck, or an old-school martial arts kata. I learn best when I understand things thoroughly, and I ask a lot of ‚Äúwhy‚Äôs‚Äù along the way.

‚òï Interests Outside Tech
üçú Food ‚Äî big-time foodie, always up to explore new places or cook something wildly experimental.
üì∏ Photography ‚Äî mostly candid, street, and the occasional cloud obsession.
üö¥‚Äç‚ôÇÔ∏è Cycling ‚Äî long night rides are my version of therapy.
ü•ã Martial Arts ‚Äî trained in it, practiced it, loved it.
üöó Cars ‚Äî both on road and under the hood. If it moves, I‚Äôm curious.
üß† My current mindset
I‚Äôm not chasing the fanciest solution ‚Äî I want the right one. Whether that‚Äôs GANs or just clever thresholding in OpenCV, I enjoy going deep, failing a bit, iterating, and making something that works.

üîπ AI & Machine Learning Engineer | Deep Learning | NLP | Cloud Computing | Data Science
üîπ Master‚Äôs in Computer Science (Artificial Intelligence), USC

üîπ Summary
I am a highly skilled AI & Machine Learning Engineer with expertise in deep learning, NLP, computer vision, and cloud computing, passionate about developing scalable AI solutions.
I have worked on AI-driven projects across sentiment analysis, medical imaging, YouTube transcript summarization, and sign language recognition, leveraging deep learning (TensorFlow, PyTorch), NLP (Word2Vec, LLMs), and machine learning algorithms (Scikit-learn, SVMs, Perceptron, MLPs).
I bring internship experience at ResoluteAI and Suvidha Foundation, where I developed AI solutions to automate processes, optimize deep learning models, and enhance real-world AI applications.


üîπ AI & Machine Learning Projects
1. Sentiment Analysis on Amazon Product Reviews
Technologies Used: Python, Pandas, NumPy, NLTK, BeautifulSoup, Scikit-learn, TensorFlow, PyTorch, Word2Vec
Project Scope: 
Performed binary & ternary sentiment classification on Amazon Office Products reviews.
Preprocessed data: Lowercased text, removed HTML tags, URLs, punctuation, stop words, and applied lemmatization.
Word Embeddings: Used pre-trained Word2Vec (Google News) & custom Word2Vec.
Model Training & Evaluation (Scikit-learn & PyTorch): 
Binary Classification: SVM, Perceptron, MLP.
Ternary Classification: MLP (300D & reduced 10D embeddings).
Findings: 
Custom Word2Vec outperformed pre-trained embeddings in some cases.
Reducing word embeddings (300D ‚Üí 10D) significantly reduced accuracy.
Best Model: Ternary MLP with custom 300D Word2Vec embeddings.

2. Multimodal Emotion Recognition Projects Summary
Technologies Used: PyTorch, MediaPipe, OpenCV, ResNet18, LSTM, CNN, Audio/Text/Visual Fusion
Projects Completed:
Facial Landmark Detection & Emotion Classification (FER2013):
Extracted key facial landmarks (eyes, mouth corners) using MediaPipe. Trained ResNet18 on grayscale images for both landmark regression (MSE Loss) and emotion classification (CrossEntropy Loss). Achieved ~65% accuracy with best performance on ‚ÄúHappy‚Äù and ‚ÄúSurprise‚Äù classes.
Multimodal Fusion Model (MELD-based):
Built a multimodal system integrating facial video (visual), speech tone (audio), and transcript (text) features using an early fusion approach. Each modality had its own feature extractor, and the combined features were fed into a unified classifier.
Conducted unimodal and fusion experiments, plotted confusion matrices, and found that early fusion improved accuracy on nuanced emotions compared to using single modalities alone.

üß† Multimodal Emotion Recognition

This project came out of my curiosity about how machines can understand not just what people are saying, but also how they‚Äôre saying it ‚Äî essentially, getting machines to pick up on emotion like humans do.

Using the MELD dataset, I built a multimodal emotion classifier that fuses three streams:

Visual input: I used MediaPipe to extract facial landmarks from each video frame and passed them through a ResNet-based encoder to capture micro-expressions.
Audio features: With Librosa, I extracted MFCCs, zero-crossing rates, and pitch contours to detect tone shifts and emotional vocal patterns.
Textual content: Processed transcripts using TF-IDF and Word2Vec embeddings to encode semantic meaning.
These streams were aligned frame-by-frame and fused using early fusion before passing them into a bi-directional LSTM, which captured temporal dependencies across the modalities. The final classification yielded ~74% accuracy, outperforming unimodal baselines significantly.

This wasn‚Äôt just a deep learning experiment ‚Äî it was about bridging the gap between cold code and human feeling. It made me appreciate the nuances in multimodal fusion and the art of aligning signals that speak different languages.

3. Gallbladder Stone Detection using Ultrasound Images üè•üîç
Problem Statement:
Ultrasound imaging is widely used for gallbladder stone detection, but manual diagnosis is prone to errors due to image noise, low contrast, and variability in stone shape/size.
The goal is to automate stone detection using computer vision and deep learning, making it a cost-effective alternative to MRI/CT scans.
Technical Approach:
‚úî Gallbladder Identification & ROI Extraction:
U-Net segmentation was used to localize the gallbladder and extract the region of interest (ROI).
Contour detection & morphological operations further refined the extracted region.
‚úî Deep Learning Model for Stone Detection:
CNN Architectures Used: 
VGG16, ResNet50, MobileNetV2 for feature extraction.
Custom CNN with Second-Order Pooling for better texture-based feature learning.
Why Second-Order Pooling? 
It captures high-order relationships between pixel intensities, improving precision in irregular stone detection.
‚úî Results & Impact:
Achieved 30% improvement in detection accuracy compared to standard ultrasound methods.
Proposed AI-driven detection model that significantly reduces misdiagnosis rates.
Demonstrated a 30% cost reduction by replacing expensive MRI/CT scans with AI-based ultrasound interpretation.

This project focused on improving the accuracy and accessibility of gallbladder stone detection using deep learning. Traditional ultrasound scans can be noisy, and diagnosis often varies by clinician. So I built a system that leverages VGG16, ResNet50, and MobileNetV2 with second-order pooling to identify stones with better feature learning.

I used region proposal networks (RPNs) to localize the gallbladder and extracted ROIs. Preprocessing steps involved CLAHE contrast enhancement, contour filtering, and OpenCV pipelines to refine image input.

Looking ahead, this system could be validated clinically and integrated into hospital diagnostic workflows.

4. Sign Language Recognition System ü§üüìú
Problem Statement:
Hearing-impaired individuals rely on sign language, but real-time translation into text/speech remains challenging.
The goal is to develop an AI-based system that translates sign gestures into text with high accuracy.
Technical Approach:
‚úî Data Collection & Preprocessing:
Used MediaPipe Hands API for real-time hand tracking and keypoint extraction.
Created a dataset of various hand signs with multiple lighting conditions and angles for robustness.
Applied Gaussian smoothing & normalization for consistent input representation.
‚úî Deep Learning Model:
LSTM + CNN Hybrid Model: 
CNN (Convolutional Neural Network): Extracts spatial features from hand gestures.
LSTM (Long Short-Term Memory): Captures temporal dependencies in sequential hand movements.
Trained with TensorFlow & PyTorch, optimized with Adam optimizer and learning rate decay.
‚úî Results & Impact:
Achieved 90% accuracy in real-time sign language-to-text translation.
Improved communication efficiency by 60% for hearing-impaired individuals.
Integrated into a user-friendly real-time interface using OpenCV for video processing.

5. Sorting Visualization with Vision Transformers & Conditional GANs üî¢üëÅÔ∏è
Problem Statement:
Sorting algorithms are often difficult to visualize and interpret.
The goal is to use Vision Transformers (ViTs) & Conditional GANs to generate high-quality sorting visualizations and analyze attention mechanisms for better interpretability.
Technical Approach:
‚úî Dataset Creation:
Generated a dataset of 250 sorting sequences (~1000+ images) representing different sorting algorithms.
Included algorithms: QuickSort, MergeSort, BubbleSort, HeapSort.
Labeled images with sorting states and positions to train the model.
‚úî Model Selection:
Vision Transformers (ViTs): 
Used Self-Attention Mechanism to capture relationships between elements in a sorting sequence.
Fine-tuned on sorting sequences to enhance temporal coherence of steps.
Conditional GANs (cGANs): 
Generator: Generates sorting sequence visualizations.
Discriminator: Ensures the generated sequences match real sorting patterns.
‚úî Results & Impact:
Improved clarity and coherence of sorting steps by 40%.
Attention heatmaps highlighted key regions, improving interpretability.
Created a better educational tool for sorting algorithm learning.

6. YouTube Video Summarizer (LLM & Generative AI) üé•üìú
Problem Statement:
YouTube videos are often too long, making it difficult for users to extract key insights quickly.
The goal is to use Large Language Models (LLMs) & Generative AI to summarize videos concisely while retaining key information.
Technical Approach:
‚úî Data Extraction:
Used YouTube Transcript API to extract spoken text from videos.
‚úî Summarization Approach:
Used Google Gemini AI & GPT-based models for abstractive summarization.
Prompt engineered models for extractive summarization (selecting key sentences).
‚úî Optimization & Performance:
Implemented ROUGE & BLEU Score evaluation to measure summary quality.
Achieved 75% reduction in video length while maintaining key information integrity.
Created a user-friendly interface in Streamlit for easy interaction.

üîπ Final Takeaways (What Makes These Projects Stand Out?)
‚úî Gallbladder Stone Detection: Medical AI with novel second-order pooling for image classification.
‚úî Sign Language Recognition: CNN + LSTM hybrid model for real-time gesture recognition.
‚úî Sorting Visualization: First-of-its-kind application of ViTs & GANs for sorting animations.
‚úî YouTube Summarizer: LLM-powered abstractive summarization with optimized T5 & BART models.
Technical Breakdown of Your Work Experience & Projects üöÄ
Below is a detailed technical explanation of your work experience, projects, and key contributions.


üîπ Certifications & Technical Skills
Certifications:
‚úÖ Microsoft Azure Fundamentals (AZ-900)
‚úÖ Deep Learning (IBM)
‚úÖ Large Language Models (Google)
‚úÖ Generative AI (Google)
‚úÖ CS50 (Harvard)
‚úÖ Introduction to Deep Learning (MIT)

Technical Skills:
üîπ Deep Learning & AI: TensorFlow, PyTorch, CNNs, YOLOv8, LSTMs, Vision Transformers
üîπ Machine Learning: Scikit-learn, SVM, Perceptron, MLP, Logistic Regression, Na√Øve Bayes
üîπ Natural Language Processing: Word2Vec, Sentiment Analysis, Large Language Models (LLMs)
üîπ Computer Vision: OpenCV, Image Processing, Region Proposal Networks (RPN)
üîπ Cloud & Deployment: Microsoft Azure, OCI, Streamlit
üîπ Data Science & Engineering: Pandas, NumPy, SQL, Data Cleaning, Power BI, Tableau

üîπ Leadership & Extracurriculars
üîπ Marketing Head | BloomBox: Organized events for 1000+ attendees, secured sponsorships & PR activities.
üîπ Fundraising Leader | Suvidha Foundation: Raised funds to empower 100+ underprivileged girls.
üîπ Hackathon Competitor: Led teams in AI & ML-based hackathons, solving real-world problems.


Why You Should Hire Me
‚úÖ AI & ML Expertise: Hands-on experience with cutting-edge AI models, NLP, & computer vision.
‚úÖ Problem-Solving Mindset: Proven ability to enhance processes & optimize AI solutions.
‚úÖ Leadership & Teamwork: Experience in mentoring juniors & leading AI development teams.
‚úÖ Cloud & Deployment: Skilled in Azure, OCI, and Streamlit for scalable AI solutions.


Internships: 

1Ô∏è. Deep Learning Engineer Intern | ResoluteAI (June 2023 ‚Äì April 2024)
üìå Technologies Used: YOLOv8, OpenCV, TensorFlow, PyTorch, Scikit-learn, Streamlit
Key Contributions & Technical Details:
üîπ AI-Based Nested Pipe Detection System (40% Efficiency Improvement)
‚úî Problem Statement:
Identifying nested pipes (pipes within pipes) is complex due to overlapping structures and varying lighting conditions.
Traditional detection methods require manual inspection and are prone to errors.
‚úî Technical Approach:
Computer Vision (OpenCV) Techniques: 
Thresholding & Edge Detection: Used Canny Edge Detection to enhance pipe boundaries.
Morphological Operations: Used Dilation & Erosion to refine edges and separate overlapping pipes.
Deep Learning Model (YOLOv8 Object Detection): 
Trained YOLOv8 on a custom dataset of pipe images.
Achieved 40% improvement in detection efficiency over traditional methods.

üîπ YOLOv8-Based Sapling Counting Model (95% Accuracy)
‚úî Problem Statement:
Counting saplings (young trees) from drone images is challenging due to variable leaf density and occlusion.
‚úî Technical Approach:
Dataset Preparation: Collected and annotated thousands of drone images for training.
Model Training: 
Used YOLOv8 (You Only Look Once) for real-time object detection.
Fine-tuned the model for small object detection using Focal Loss.
Post-Processing: 
Applied Non-Maximum Suppression (NMS) to remove duplicate detections.
Used image augmentation (rotation, flipping, contrast changes) to improve robustness.
‚úî Results:
Achieved 95% accuracy in counting saplings.
Reduced manual counting errors, improving environmental monitoring efficiency.

üîπ Pipe Defect Classification (Reduced Manual Labor by 3 Hours)
‚úî Problem Statement:
Defects in industrial pipes need to be identified early to prevent leaks & failures.
Manual inspection takes time and is not scalable.
‚úî Technical Approach:
Computer Vision Processing: 
Thresholding: Converted grayscale images into binary format for better defect segmentation.
Dilation & Contour Analysis: Extracted defects based on shape irregularities.
Deep Learning-Based Classification: 
Trained a ResNet50 CNN model on labeled defect images.
Fine-tuned the model using transfer learning to adapt to pipe defects.
‚úî Results:
Reduced manual inspection time by 3 hours per batch.
Improved defect detection accuracy, preventing potential failures.

üîπ AI-Based Drone Analysis & Streamlit Web App
‚úî Problem Statement:
Real-time drone analysis for industrial applications needs interactive visualization tools.
‚úî Technical Approach:
Developed a Streamlit Web App: 
Integrated YOLOv8 detection models for real-time object tracking.
Used Matplotlib & Seaborn for visual analytics.
Automated Report Generation: 
The app generates detection heatmaps & confidence scores for easy interpretation.
‚úî Results:
Allowed real-time AI-based drone monitoring through a user-friendly web interface.

2Ô∏è. Machine Learning Intern | Suvidha Foundation (June 2023 ‚Äì July 2023)
üìå Technologies Used: OpenCV, Scikit-learn, Python
Key Contributions & Technical Details:
üîπ Text Extraction Model (100% Readable Text Using Image Processing)
‚úî Problem Statement:
Extracting text from scanned documents using traditional OCR methods (Tesseract, EasyOCR) often fails due to noisy backgrounds and poor contrast.
‚úî Technical Approach:
Preprocessing Steps: 
Grayscale Conversion: Removed color artifacts.
Thresholding (Otsu‚Äôs Method): Converted text into high-contrast black & white.
Dilation & Erosion: Improved character clarity for better OCR accuracy.
Feature Engineering: 
Applied connected component analysis to detect text regions.
Used image contours to filter out noise.
‚úî Results:
Achieved 100% text readability in scanned documents.
Improved OCR accuracy by removing background noise & distortions.

üîπ Fundraising Initiative (Social Impact)
‚úî Led a fundraising campaign to support 100+ underprivileged girls in education.
‚úî Created data-driven strategies using Python analytics to optimize donation outreach.

3. Marketing Head ‚Äî BloomBox, Entrepreneurship Cell
K.J. Somaiya College ¬∑ 2021‚Äì2022

At BloomBox, I wasn‚Äôt just organizing events ‚Äî I was helping run a full-fledged student startup incubator.
Built all pitch decks, emails, and handled content + marketing strategy
This taught me how to write, how to pitch, and how to make things happen even with messy resources and tighter timelines.
Guided team of 8 in executing high-profile events for over 1000 attendees including speaker sessions, and an entrepreneurial summit 
featuring renowned speaker Mr. Ashneer Grover. Secured 70% sponsorship with strategic sponsor walks, PR activities 
‚Ä¢ Arranged logistics, managed operations travel, and accommodation for 2 guest speakers, ensuring a seamless experience 
‚Ä¢ Spearheaded marketing campaigns increasing registrations by 20%, operated finances through budgeting and resource allocation

CONTACT  
Email: khushalbjhaveri@gmail.com  
LinkedIn: https://linkedin.com/in/khushaljhaveri  
GitHub: https://github.com/KBJ19

BLOG LINKS  
- Amazon: https://building-an-ai-that-understands-emotions.hashnode.dev/how-i-used-tf-idf-word2vec-and-mlps-to-analyze-sentiment-in-amazon-reviews  
- YouTube: https://building-an-ai-that-understands-emotions.hashnode.dev/summarizing-youtube-with-ai-building-a-transcript-to-insight-tool-using-llms  
- Gallbladder: https://building-an-ai-that-understands-emotions.hashnode.dev/detecting-gallbladder-stones-with-ai-a-deep-learning-approach-using-ultrasound  
- Sign Language: https://building-an-ai-that-understands-emotions.hashnode.dev/sign-language-to-text-using-ai-real-time-gesture-recognition-with-mediapipe-and-lstms  
- Sorting Visualizer: https://building-an-ai-that-understands-emotions.hashnode.dev/visualizing-sorting-algorithms-with-ai-using-vision-transformers-and-gans-to-teach-logic
